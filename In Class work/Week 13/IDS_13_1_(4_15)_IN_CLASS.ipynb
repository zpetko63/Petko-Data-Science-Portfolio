{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning & KMeans Clustering\n",
        "\n",
        "This notebook demonstrates the use of KMeans clustering—an unsupervised learning technique—to discover inherent groupings in the Breast Cancer dataset. We then compare the clustering assignments with the known labels to assess the algorithm's ability to recover meaningful groups.\n",
        "\n",
        "In this notebook, we cover:\n",
        "\n",
        "1. **Overview of Unsupervised Learning & KMeans**  \n",
        "   - Introduction to clustering when there is no response variable.\n",
        "   - Explanation of how KMeans partitions data into k clusters by minimizing within-cluster variance.\n",
        "   \n",
        "2. **Data Preprocessing**  \n",
        "   - Loading the Breast Cancer dataset.\n",
        "   - Centering and scaling the features, which is crucial for KMeans performance.\n",
        "\n",
        "3. **KMeans Clustering Computation and Visualization**  \n",
        "   - Running KMeans on the standardized data.\n",
        "   - Visualizing the clustering results in 2D using PCA for dimensionality reduction.\n",
        "   - Comparing the cluster assignments with the true labels (benign vs. malignant).\n",
        "\n",
        "4. **Evaluation and Discussion**  \n",
        "   - Assessing clustering performance with metrics (e.g., Adjusted Rand Index).\n",
        "   - Discussing the insights and potential limitations."
      ],
      "metadata": {
        "id": "1UTF4P2-H2Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "lbT7ieBzH4ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load and Preprocess the Data\n",
        "\n",
        "We begin by loading the Breast Cancer dataset.\n",
        "- The Breast Cancer dataset contains measurements for benign and malignant tumors.\n",
        "- Here, the dataset provides:\n",
        "    - `X` as a feature matrix\n",
        "    - `y` as a target variable"
      ],
      "metadata": {
        "id": "mv2jCW3SH6By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer Wisconsin dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data  # Feature matrix\n",
        "y = breast_cancer.target  # Target variable (diagnosis)\n",
        "feature_names = breast_cancer.feature_names\n",
        "target_names = breast_cancer.target_names\n",
        "\n",
        "print(y)\n",
        "pd.DataFrame(X, columns = [feature_names])"
      ],
      "metadata": {
        "id": "fIf3HAtbH6V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling is critical here because KMeans relies on distance calculations and can be biased by the scale of features."
      ],
      "metadata": {
        "id": "hIk0LW21JMdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# It is important to center and scale the features since PCA is sensitive to the variable scales.\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "3D1i01ZjH7E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Compute KMeans Clustering\n",
        "KMeans clustering partitions the data into k clusters by iteratively assigning points to the nearest cluster centroid and then updating the centroids based on the cluster’s mean.\n",
        "\n",
        "Key Concepts:\n",
        "- Initialization: Randomly select k centroids.\n",
        "\n",
        "- Assignment & Update: Reassign points and recalculate centroids until convergence.\n",
        "\n",
        "- Choosing k: For the Breast Cancer dataset (with two known classes), we set k = 2."
      ],
      "metadata": {
        "id": "U9HBQUidH6-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Set the number of clusters to 2, as we have two classes (malignant and benign)\n",
        "\n",
        "\n",
        "# Output the centroids and first few cluster assignments\n"
      ],
      "metadata": {
        "id": "UC9Yh2GcIrva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Visualization of KMeans Clustering Results\n",
        "Visualization helps us understand how well KMeans has partitioned the data. However, our dataset is high-dimensional, so we first reduce it to 2 dimensions using PCA for visualization. We then plot the PCA scores with colors corresponding to the cluster assignments."
      ],
      "metadata": {
        "id": "yrXtHn_-IzXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3a. 2D Scatter Plot of Clustering Results Using PCA"
      ],
      "metadata": {
        "id": "G5IpLaJ5qxM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce the data to 2 dimensions for visualization using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Create a scatter plot of the PCA-transformed data, colored by KMeans cluster labels\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[clusters == 0, 0], X_pca[clusters == 0, 1],\n",
        "            c='navy', alpha=0.7, edgecolor='k', s=60, label='Cluster 0')\n",
        "plt.scatter(X_pca[clusters == 1, 0], X_pca[clusters == 1, 1],\n",
        "            c='darkorange', alpha=0.7, edgecolor='k', s=60, label='Cluster 1')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('KMeans Clustering: 2D PCA Projection')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z1XOZXFNI6nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3b. Comparing Clusters with True Labels\n",
        "Even though KMeans is unsupervised, we can compare its cluster assignments with the actual labels to gauge performance."
      ],
      "metadata": {
        "id": "2eGyOo-DJypa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For comparison, visualize true labels using PCA (same 2D projection)\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['navy', 'darkorange']\n",
        "for i, target_name in enumerate(target_names):\n",
        "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
        "                color=colors[i], alpha=0.7, edgecolor='k', s=60, label=target_name)\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('True Labels: 2D PCA Projection')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nCmmkqiHIzB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "RwT4HkUSH6zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluation and Discussion\n",
        "Although KMeans clustering is unsupervised, we can assess how well the clusters match the true labels.\n",
        "- There are other metrics like Adjusted Rand Index (ARI) that are also used to evaluate clusters to true values."
      ],
      "metadata": {
        "id": "NPL8MykLJ8Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Since KMeans labels are arbitrary (e.g., 0 and 1) and may not match the true labels directly,\n",
        "# we compute accuracy for both the original labels and their complement, and choose the higher value.\n"
      ],
      "metadata": {
        "id": "u8_jgmgKH6sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Evaluating the Best Number of Clusters\n",
        "Determining the optimal number of clusters is a common challenge in clustering applications. Two popular methods to address this are:\n",
        "\n",
        "- Elbow Method:\n",
        "We plot the Within-Cluster Sum of Squares (WCSS) against different values of k. The \"elbow\" point—where the rate of decrease sharply changes—suggests an optimal value for k.\n",
        "\n",
        "- Silhouette Score:\n",
        "This score quantifies how similar an object is to its own cluster compared to other clusters. A higher silhouette score indicates better clustering. We compute the average silhouette score for different values of k and select the one with the highest score."
      ],
      "metadata": {
        "id": "s0w-A48RH6lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the range of k values to try\n",
        "# starting from 2 clusters to 10 clusters\n",
        "\n",
        "# Within-Cluster Sum of Squares for each k\n",
        "# Silhouette scores for each k\n",
        "\n",
        "# Loop over the range of k values"
      ],
      "metadata": {
        "id": "LyzaCUFCr3F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the Results Above"
      ],
      "metadata": {
        "id": "bSvQBGorstwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the Elbow Method result\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(ks, wcss, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot the Silhouette Score result\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ks, silhouette_scores, marker='o', color='green')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score for Optimal k')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2OPpZL6xssBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Classification\n",
        "In this notebook, we will walk through building a Decision Tree classifier\n",
        "using the Titanic dataset. Decision trees are non-parametric models that\n",
        "can capture nonlinear relationships by recursively splitting the data.\n",
        "____\n",
        "We'll cover:\n",
        "1. Loading and inspecting the dataset.\n",
        "2. Preprocessing: Handling missing values and encoding categorical data.\n",
        "3. Splitting the data into training and testing sets.\n",
        "4. Training a Decision Tree model.\n",
        "5. Evaluating the model's performance with accuracy, confusion matrix, and a classification report.\n",
        "6. Visualizing the decision tree.\n",
        "7. Analyzing model performance using ROC curve and AUC."
      ],
      "metadata": {
        "id": "SAms_E00CSST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load and Inspect the Data**\n",
        "\n",
        "We use the Titanic dataset available from seaborn, which includes details about passengers. This dataset is widely used for classification tasks."
      ],
      "metadata": {
        "id": "lImPR-PAChUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dBYPNotuwfd"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset in seaborn\n",
        "data = sns.load_dataset('titanic')\n",
        "\n",
        "# Inspect the dataset\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Data Preprocessing**\n",
        "\n",
        "Our next step is to prepare the data for modeling:\n",
        "\n",
        "- **Handling Missing Values:**\n",
        "   Here, the line for dropping rows with missing 'age' is commented out.\n",
        "   Depending on your needs, you might choose to drop these rows or use imputation.\n",
        "\n",
        "- **Encoding Categorical Variables:**\n",
        "   Decision tree algorithms can handle numerical inputs, so we convert\n",
        "   categorical variables (e.g., `sex`) into numeric format using one-hot encoding.\n",
        "\n",
        "*Note: We use drop_first=True to avoid the dummy variable trap.*"
      ],
      "metadata": {
        "id": "20rN0ZYCCmLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values (optional for decision trees)\n",
        "#data.dropna(subset=['age'], inplace=True)\n",
        "\n",
        "# Encoding categorical variables\n",
        "df = pd.get_dummies(data, columns=['sex'], drop_first=True) # Use drop_first = True to avoid \"dummy trap\"\n",
        "\n",
        "# Define features and target\n",
        "features = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex_male']\n",
        "X = df[features]\n",
        "y = df['survived']\n",
        "\n",
        "# Preview the cleaned dataset\n",
        "print(X.head())\n",
        "print(y.head())"
      ],
      "metadata": {
        "id": "Xtx_7MlTvIcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Splitting the Data**\n",
        "\n",
        "We split the dataset into training and testing sets. The training set is used to build the decision tree model, while the testing set is used to evaluate its performance."
      ],
      "metadata": {
        "id": "3OmbakPvCzxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "yblwppiF7ya3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Train the Decision Tree Model**\n",
        "\n",
        "We initialize and train a Decision Tree classifier.\n",
        "**Why Decision Trees?**\n",
        "- They are intuitive and easy to interpret.\n",
        "- They capture non-linear relationships without needing feature scaling.\n",
        "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) on sklearn.\n",
        "\n",
        "Here, we'ss use default parameters at first, but tuning (e.g., max_depth, min_samples_split) can improve performance and prevent overfitting."
      ],
      "metadata": {
        "id": "O0CcE1FzC8bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize and train tree classification model\n"
      ],
      "metadata": {
        "id": "n1MnU_HRu2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Evaluate the Model**\n",
        "\n",
        "We now assess our model’s performance on the test data using several metrics:\n",
        "\n",
        "- **Accuracy:** The overall proportion of correct predictions.\n",
        "- **Confusion Matrix:** Displays the number of correct and incorrect predictions.\n",
        "- **Classification Report:** Provides precision, recall, and F1-score, which help in understanding performance per class."
      ],
      "metadata": {
        "id": "n0UT24sfDEpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Display classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "lPJLnzvu8DkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Visualizing the Decision Tree**\n",
        "\n",
        "One of the advantages of decision trees is their interpretability. We can visualize the tree structure using the graphviz library.\n",
        "The visualization shows:\n",
        "- Splitting criteria at each node.\n",
        "- Feature names used for splits.\n",
        "- Class distributions within the nodes."
      ],
      "metadata": {
        "id": "rG-kkOyZDKAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import graphviz and export the decision tree to dot format for visualization\n",
        "\n",
        "\n",
        "# Generate and display the decision tree graph\n"
      ],
      "metadata": {
        "id": "nXXsbI-39i_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7: ROC Curve and AUC Analysis**\n",
        "The ROC (Receiver Operating Characteristic) curve helps evaluate the model’s\n",
        "performance across different classification thresholds:\n",
        "\n",
        "- **ROC Curve:** Plots True Positive Rate (TPR) against False Positive Rate (FPR).\n",
        "- **AUC (Area Under the Curve):** Summarizes the overall ability of the model to discriminate between classes.\n",
        "\n",
        "Here, we calculate and plot the ROC curve along with the AUC score."
      ],
      "metadata": {
        "id": "SuxA6d16DpNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ROC curve metrics\n",
        "\n",
        "# Get the predicted probabilities for the positive class (survival)\n",
        "\n",
        "# Calculate the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds\n",
        "\n",
        "\n",
        "# Compute the Area Under the Curve (AUC) score\n",
        "\n",
        "\n",
        "# Plot the ROC curve\n"
      ],
      "metadata": {
        "id": "s6OQE2-T9_Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 8: Train a Logistic Regression Model**\n",
        "\n",
        "Call it `lr_model`"
      ],
      "metadata": {
        "id": "JQM8vKwVNN8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize and train logistic regression model\n"
      ],
      "metadata": {
        "id": "-MemThnGNUxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Adding a ROC Curve for the Logistic Regression Model\n",
        "- Copy and paste the code from Step 7.\n",
        "- Replace `model` with the logistic regression model.\n",
        "- Append \"`_lr`\" to the `y_probs`, `fpr`, `tpr`, `thresholds`, and `roc_auc` variables.\n",
        "- Keep the decision trees ROC curve plot, but add a new `plt.plot()` with the new logistic regression variables."
      ],
      "metadata": {
        "id": "-p543dwoWRm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted probabilities for the positive class (survival)\n",
        "\n",
        "# Calculate the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds\n",
        "\n",
        "# Compute the Area Under the Curve (AUC) score\n",
        "\n",
        "# Plot the ROC curve\n"
      ],
      "metadata": {
        "id": "mOBZzzvZRRm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
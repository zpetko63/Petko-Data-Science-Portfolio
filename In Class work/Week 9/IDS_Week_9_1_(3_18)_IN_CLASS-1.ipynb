{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "In this notebook, we'll walk step-by-step through performing logistic regression using the Titanic dataset. We'll cover each of the following steps clearly and thoroughly:\n",
        "\n",
        "1. Loading and inspecting the dataset.\n",
        "2. Preprocessing: Handling missing values and encoding categorical data.\n",
        "3. Splitting the data into training and testing sets.\n",
        "4. Training a logistic regression model.\n",
        "5. Evaluating the model's performance.\n",
        "6. Checking assumptions for logistic regression.\n",
        "7. Interpreting the model's results clearly."
      ],
      "metadata": {
        "id": "lKusDZoYu3Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Load and Inspect the Data**\n",
        "We'll use the Titanic dataset, a popular dataset for predicting passenger survival based on various features."
      ],
      "metadata": {
        "id": "MijBWi3T6Hf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dBYPNotuwfd"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Titanic dataset in seaborn\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Inspect the dataset\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Preprocess the Data**\n",
        "Before training our model, we must prepare the dataset. This step involves two key processes:\n",
        "\n",
        "*2.1 Handle Missing Values*\n",
        "Logistic regression requires datasets without missing values, so we remove rows with missing data in important columns (age, embarked).\n",
        "\n",
        "*2.2 Encode Categorical Variables*\n",
        "Machine learning algorithms require numerical inputs. Therefore, categorical variables (sex, embarked) must be converted into numeric form using one-hot encoding.\n",
        "\n",
        "- Why encoding?\n",
        "    - Converts categorical labels into numeric values that the model can interpret.\n",
        "    - Avoids misinterpretation of categorical variables as numeric variables (e.g., treating \"male\" as numerically greater or less than \"female\")."
      ],
      "metadata": {
        "id": "aSakmgEGvE78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "\n",
        "# Encoding categorical variables\n",
        "# Use drop_first = True to avoid \"dummy trap\"\n",
        "\n",
        "# Define features and target\n",
        "\n",
        "\n",
        "# Preview the cleaned dataset\n"
      ],
      "metadata": {
        "id": "Xtx_7MlTvIcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3: Split the Data**\n",
        "We divide our dataset into two subsets:\n",
        "- *Training Set*: To build our logistic regression model.\n",
        "- *Testing Set*: To evaluate the performance of our model."
      ],
      "metadata": {
        "id": "Hpc_vEn37QSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split dataset into training and testing subsets\n"
      ],
      "metadata": {
        "id": "yblwppiF7ya3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Train the Logistic Regression Model**\n",
        "We'll now build and train our logistic regression model using the training data."
      ],
      "metadata": {
        "id": "YKlqE7L676_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize and train logistic regression model\n"
      ],
      "metadata": {
        "id": "n1MnU_HRu2iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5: Evaluate the Model**\n",
        "To understand how well our model predicts survival, we use the following evaluation metrics:\n",
        "- *Accuracy*: Proportion of correct predictions.\n",
        "- *Confusion Matrix*: Breakdown of predictions (True positives, True negatives, False positives, False negatives).\n",
        "- *Classification Report*: Precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "vQWLzTL58K2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Predict on test data\n",
        "\n",
        "# Calculate accuracy\n",
        "\n",
        "\n",
        "# Generate confusion matrix\n",
        "\n",
        "# Display classification report\n"
      ],
      "metadata": {
        "id": "lPJLnzvu8DkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Examining and Interpreting the Model**\n",
        "After training our logistic regression model, let's examine the coefficients to understand their meaning in more detail.\n",
        "- Coefficients in logistic regression represent the effect of each feature on the log-odds of the target (survival in this case).\n",
        "- Positive coefficients increase the odds of the outcome (survival).\n",
        "- Negative coefficients decrease the odds of the outcome (survival).\n",
        "- The magnitude of coefficients shows the strength of each feature's influence."
      ],
      "metadata": {
        "id": "bsGKbVbU9X6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Interpreting Coefficients\n",
        "After training our logistic regression model, let's examine the coefficients to understand their meaning in more detail.\n",
        "- Coefficients in logistic regression represent the effect of each feature on the log-odds of the target (survival in this case).\n",
        "- Positive coefficients increase the odds of the outcome (survival).\n",
        "- Negative coefficients decrease the odds of the outcome (survival).\n",
        "- The magnitude of coefficients shows the strength of each feature's influence.\n",
        "\n",
        "Output:\n",
        "- Coefficient Interpretation:\n",
        "    - Positive coefficients: Increase the log-odds (and thus probability) of survival.\n",
        "    - Negative coefficients: Decrease the log-odds (and thus probability) of survival.\n",
        "\n",
        "\n",
        "\n",
        "| Feature      | Coefficient | Impact on Survival Probability                      | Explanation                                                    |\n",
        "|--------------|-------------|-----------------------------------------------------|----------------------------------------------------------------|\n",
        "| `pclass`     | -1.21       | Decreases                                           | Lower passenger classes (higher numeric values) decrease survival odds.            |\n",
        "| `age`        | -0.04       | Slight negative impact                              | Older passengers have slightly lower odds of survival.         |\n",
        "| `sibsp`      | -0.35       | Negative impact                                     | Having more siblings/spouses aboard decreases survival odds.   |\n",
        "| `parch`      | -0.05       | Slight negative impact                              | Having more parents/children aboard slightly reduces odds.     |\n",
        "| `fare`       | 0.002      | Very slight positive impact                         | Paying higher fares slightly increases survival odds.          |\n",
        "| `sex_male`   | -2.61       | Strong negative impact                              | Being male greatly decreases the probability of survival.      |\n",
        "| **Intercept**| 5.47        | Baseline log-odds                                   | Baseline survival odds for females with lowest-class, fare, age, ect. |\n"
      ],
      "metadata": {
        "id": "JeALrss89wCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract coefficients and intercept\n",
        "\n",
        "\n",
        "# Display coefficients\n"
      ],
      "metadata": {
        "id": "nXXsbI-39i_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Understanding the `predict_proba` Function\n",
        "Logistic regression outputs probabilities between 0 and 1, indicating the likelihood of belonging to a specific class.\n",
        "- The function predict_proba() returns two columns:\n",
        "    - Probability of class 0 (not survived).\n",
        "    - Probability of class 1 (survived).\n",
        "- Output:\n",
        "    - Each row gives the probability of not surviving (first column) and surviving (second column).\n",
        "    - Useful for making informed decisions based on probabilities rather than just binary predictions."
      ],
      "metadata": {
        "id": "EZEykx279225"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities for the test set\n",
        "\n",
        "\n",
        "# Display probabilities for first 5 test observations\n"
      ],
      "metadata": {
        "id": "s6OQE2-T9_Rf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}